---
title: "Análisis de un conjunto de datos de origen biológico mediante técnicas de machine learning supervisadas y no supervisadas"
author: "Carmen Palacios Clar, Lidia Sanchez, Laura, Alba Garcia Moya"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    toc: true
    toc_float: true
    fig_caption: true
    css: logos_css/usr_styles.css
  word_document:
    toc: true
  pdf_document:
    toc: true
subtitle: "Actividad 3. Algoritmos e Inteligencia Articifial"
params:
  mostra: true
always_allow_html: true
---
 
```{=html}
<script>
$(document).ready(function() {
  $head = $('#header');

  $head.prepend('<img src=\"logos_css/Logo_UNIR.png" alt="Logo_UNIR" style="display: block; margin: 0 auto; width: 185px;">');
});
</script>

```
# Objetivo
El objetivo de esta actividad es implementar de forma razonada técnicas de aprendizaje supervisado y no supervisado para el análisis de un conjunto de datos de origen biológico. 

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#setwd("/Users/carme/OneDrive/Escritorio/R/Master_bioinformatica/Algoritmos/actividades/Act3_IA")

#Preparación del entorno de trabajo

#Cargar librerias
library(glmnet) # ElasticNet
library(tidyverse)
library(caret) # ML
library(rpart) # DT
library(rpart.plot) # DT plot
library(rattle) # DT plot
library(pROC) # ROC
library(PRROC) # PR-Curve
library(gridExtra) # juntar los gráficos
library(dplyr)
library(MASS)
library(class)
library(klaR)
library(rpart)

#Cargar dataset
clases <- read.csv("Datos/classes.csv", header=F, sep = ";")
gene_expression_names <- read.csv("Datos/gene_expression.csv", header=F, sep = ";", col.names = read_lines("Datos/column_names.txt"))

```


DEPURADO DE LOS DATOS
###    1. Procesamiento de los datos (0,5 puntos):


##### ¿Qué método habéis escogido para llevar a cabo la imputación de los datos? Razonad vuestra respuesta. (0,3 puntos).
El método escogido.....

##### ¿Habéis llevado a cabo algún otro tipo de procesamiento? Razonad vuestra respuesta. (0,2 puntos).



```{r depurar, include=FALSE}

#Nombre de las columnas

colnames(gene_expression) <- read_lines("Datos/column_names.txt")

# Asignar nombres a las columnas de classes

colnames(classes) <- c("Muestra", "Clase")

# Datos NA 
sumas <- colSums(gene_expression) 
columnascero <- names(sumas[sumas==0])
gene_expression <- gene_expression[, !names(gene_expression) %in% columnascero] 

# Unir archivos

dataframe <-cbind(gene_expression, classes)


# Mover columnas clase y muestra
dataframe <- dataframe %>%
  dplyr::select(Muestra, Clase, everything())

#Escalado de datos
data_scaled <- dataframe %>%
  mutate(across(where(is.numeric), scale))

# Quitamos la columna Muestra
data_scaled <- data_scaled %>% dplyr::select(-Muestra)


#Filtrado de datos columnas de varianza cero
filt_data_scaled <- data_scaled[,-nearZeroVar(data_scaled)]


```

# Implementación de cuatro métodos de aprendizaje no supervisado

## Modelos No Supervisados

### Reducción de Dimensionalidad

# Carga de librerías necesarias
library(tidyverse)
library(ggplot2)
library(Rtsne)
library(cluster)
library(factoextra)

# PCA se elige porque permite identificar las combinaciones lineales de variables que
# explican la mayor variabilidad en los datos, facilitando la visualización y reduciendo
# la dimensionalidad de manera eficiente. Como nuestro dataset parte de una gran dimensionalidad 
# de variables, este método nos permite reducirlo, para explicar el porcentage de varianza deseado.

# Cargar datos
expresion_genica <- read.csv("gene_expression.csv", row.names = 1)
clases <- read.csv("classes.csv", row.names = 1)

# Unir los datos con las clases
expresion_genica$Clase <- clases$Clase

# Normalización de los datos
expresion_genica_scaled <- scale(expresion_genica[, -ncol(expresion_genica)])

# PCA
pca_result <- prcomp(expresion_genica_scaled, center = TRUE, scale. = TRUE)

# Visualización de PCA
ggplot(data.frame(PC1 = pca_result$x[,1], PC2 = pca_result$x[,2], Clase = expresion_genica$Clase),
       aes(x = PC1, y = PC2, color = Clase)) +
  geom_point() +
  labs(title = "PCA de expresión génica")

# t-SNE se usa porque es una técnica no lineal adecuada para captar relaciones complejas
# en los datos y mejorar la separación de clases en espacios de baja dimensión.

# t-SNE
tsne_result <- Rtsne(expresion_genica_scaled, perplexity = 30, verbose = TRUE, max_iter = 500)
tsne_data <- data.frame(Dim1 = tsne_result$Y[,1], Dim2 = tsne_result$Y[,2], Clase = expresion_genica$Clase)

# Visualización de t-SNE
ggplot(tsne_data, aes(x = Dim1, y = Dim2, color = Clase)) +
  geom_point() +
  labs(title = "t-SNE de expresión génica")

### Clusterización

# K-means se usa porque es un algoritmo muy fácil para dividir datos en grupos,
# ideal para conjuntos de datos grandes con distribución bien definida.

# K-means
set.seed(123)
kmeans_result <- kmeans(expresion_genica_scaled, centers = 3, nstart = 25)
expresion_genica$Cluster <- as.factor(kmeans_result$cluster)

# Visualización K-means
ggplot(data.frame(PC1 = pca_result$x[,1], PC2 = pca_result$x[,2], Cluster = expresion_genica$Cluster),
       aes(x = PC1, y = PC2, color = Cluster)) +
  geom_point() +
  labs(title = "Clustering K-means sobre PCA")

# La clusterización jerárquica permite observar la estructura de los datos y determinar
# el número de clústeres basado en dendrogramas, lo que proporciona más flexibilidad
# en comparación con K-means.

# Clusterización Jerárquica
dist_matrix <- dist(expresion_genica_scaled)
hclust_result <- hclust(dist_matrix, method = "ward.D2")

# Dendrograma
fviz_dend(hclust_result, k = 3, rect = TRUE)


# Implementación de tres métodos de aprendizaje supervisado 

```{r, echo=FALSE, warning=FALSE, message=FALSE}

#Preparación para el trabajo con métodos supervisados

dataframe$Clase <- as.factor(dataframe$Clase)

set.seed(1995)

#División del conjunto de datos filtrados: entrenamiento (80%) y prueba (20%)
filt_data_scaled$Clase <- as.factor(filt_data_scaled$Clase)
train_index <- createDataPartition(filt_data_scaled$Clase, p = 0.8, list = FALSE)

train_data <- filt_data_scaled[train_index, ]
test_data <- filt_data_scaled[-train_index, ]

train_data$Clase <- factor(train_data$Clase, levels = c("AGH", "CFB", "CGC", "CHC", "HPB"))
test_data$Clase <- factor(test_data$Clase, levels = c("AGH", "CFB", "CGC", "CHC", "HPB"))

```

Después de comprobar los modelos supervisados: *LDA, QDA, RDA, kNN, SMV lineal y árbol de decisión*, se deciden implementear los 3 siguientes:

# SVM lineal (Support Vector Machine)

```{r, echo=FALSE, warning=FALSE, message=FALSE}

#SVM (Support Vector Machine)

library(caret)
  
  
#Modelo de SVM lineal utilizando el paquete caret
svmModelLineal <- train(Clase ~.,
                        data = train_data,
                        method = "svmLinear",
                        trControl = trainControl(method = "cv", number = 10),
                        preProcess = c("center", "scale"),
                        tuneGrid = expand.grid(C = seq(0, 2, length = 20)), 
                        prob.model = TRUE) 
svmModelLineal

plot(svmModelLineal) 

#En el gráfico se observa que el valor del cost mejor está cercano a 1.5 (C = 1.368421).

  
#Realizar predicciones en el conjunto de prueba utilizando el modelo entrenado
predictions <- predict(svmModelLineal, newdata = test_data )
predictions
  
#Evaluar la precisión del modelo utilizando la matriz de confusión
smvModelLineal_conf_matrix <- confusionMatrix(predictions, test_data$Clase)
print(smvModelLineal_conf_matrix)


#SVM lineal
probabilities_svm_linear <- predict(svmModelLineal, newdata = test_data, type = "prob")
probabilities_svm_linear

```
# Descripción de la Matriz de Confusión 

La matriz de confusión muestra el número de predicciones correctas e incorrectas para cada clase:

- AGH: 29 casos fueron correctamente clasificados como AGH. No hubo errores.
- CFB: 60 casos fueron correctamente clasificados como CFB. Hubo 2 falsos positivos (se predijo CFB cuando era CGC).
- CGC: 28 casos fueron correctamente clasificados como CGC. Hubo 16 falsos positivos (se predijo CGC cuando era CHC) y 8 falsos positivos (se predijo CGC cuando era HPB).
- CHC: 9 casos fueron correctamente clasificados como CHC.
- HPB: 7 casos fueron correctamente clasificados como HPB.

## Métricas de evaluación

##### Accuracy (Exactitud)

La exactitud global del modelo es del 83.65%, lo que indica que clasificó correctamente el 83.65% de todos los casos. El intervalo de confianza del 95% (0.7697, 0.8903) sugiere na confianza moderada en esta estimación. Notablemente menor que el modelo anterior.

##### Kappa: 

El coeficiente Kappa de 0.7815 indica una buena concordancia entre las predicciones del modelo y las etiquetas reales, aunque no tan alta como el modelo anterior.

##### Valor P [Exactitud > Tasa de No Información]

El valor p < 2.2e-16 es extremadamente significativo. Esto indica que la exactitud del modelo es significativamente superior a la tasa de no información (.3774).

## Métricas por clase

##### Sensibilidad (Recall):

- AGH: 100% (todos los casos AGH reales fueron identificados correctamente).
- CFB: 100% (todos los casos CFB reales fueron identificados correctamente).
- CGC: 100% (todos los casos CGC reales fueron identificados correctamente).
- CHC: 33.33% (solo un tercio de los casos CHC reales fueron identificados correctamente).
- HPB: 46.67% (menos de la mitad de los casos HPB reales fueron identificados correctamente).

##### Especificidad:

-  AGH: 100% (todos los casos que no eran AGH fueron clasificados correctamente como no AGH).
-  CFB: 97.98% (casi todos los casos que no eran CFB fueron clasificados correctamente como no CFB).
-  CGC: 81.68% (un número considerable de casos que no eran CGC fueron incorrectamente clasificados como CGC).
-  CHC: 100% (todos los casos que no eran CHC fueron clasificados correctamente como no CHC).
-  HPB: 100% (todos los casos que no eran HPB fueron clasificados correctamente como no HPB).

##### Valor Predictivo Positivo (PPV):

- AGH: 100% (todos los casos predichos como AGH realmente eran AGH).
- CFB: 96.77% (casi todos los casos predichos como CFB realmente eran CFB).
- CGC: 53.85% (algo más de la mitad de los casos predichos como CGC realmente eran CGC).
- CHC: 100% (todos los casos predichos como CHC realmente eran CHC).
- HPB: 100% (todos los casos predichos como HPB realmente eran HPB).

##### Valor Predictivo Negativo (NPV):

- AGH: 100% (todos los casos predichos como no AGH realmente no eran AGH).
- CFB: 100% (todos los casos predichos como no CFB realmente no eran CFB).
- CGC: 100% (todos los casos predichos como no CGC realmente no eran CGC).
- CHC: 88% (la mayoría de los casos predichos como no CHC realmente no eran CHC).
- HPB: 94.74% (casi todos los casos predichos como no HPB realmente no eran HPB).

## Resumen general:

El modelo tiene buen rendimiento general, pero presenta dificultades en la identificación de las clases CHC y HPB, lo que podría afectar su aplicabilidad en escenarios donde estas clases son críticas. 

# Modelo kNN (k-Nearest Neighbors)

```{r, echo=FALSE, warning=FALSE, message=FALSE}

library(caret)

#Crear un modelo de k-NN utilizando el paquete caret
knnModel <- train(Clase ~ .,
                  data = train_data,
                  method = "knn",
                  trControl = trainControl(method = "cv", number = 10),
                  preProcess = c("center", "scale"),
                  tuneLength = 30)
knnModel
# Accuracy was used to select the optimal model using the largest value. The final value used for the model was k = 15.

plot(knnModel)
#A mayor número de vecinos peor va clasificando. En la gráfica vemos que k=15 es óptimo.

predictions <- predict(knnModel, newdata = test_data )
predictions

#Evaluar la precisión del modelo utilizando la matriz de confusión
knn_conf_matrix <- confusionMatrix(probabilities_knn, test_data$Clase)
print(knn_conf_matrix)

# Obtener probabilidades
probabilities_knn <- predict(knnModel, newdata = test_data, type = "prob")
probabilities_knn

```
# Descripción de la Matriz de Confusión 

 La matriz de confusión muestra el número de predicciones correctas e incorrectas para cada clase:

 - AGH: 28 casos fueron correctamente clasificadas como AGH. Hubo 1 falso negativo (se predijo CHC cuando era AGH).
 - CFB: 60 casos fueron correctamente clasificadas como CFB. No hubo errores.
 - CGC: 28 casos fueron correctamente clasificadas como CGC. Hubo 1 falso positivo (se predijo CGC cuando era HPB).
 - CHC: 27 casos fueron correctamente clasificadas como CHC. Hubo 1 falso negativo (se predijo AGH cuando era CHC).
 - HPB: 14 casos fueron correctamente clasificadas como HPB. No hubo errores.

## Métricas de evaluación

##### Accuracy (Exactitud): 

La exactitud global del modelo es del 98.74%, lo que indica que clasificó correctamente el 98.74% de todos los casos. El intervalo de confianza del 95% (0.9553, 0.9985) sugiere una alta confianza en esta estimación.

##### Kappa: 

El coeficiente Kappa de 0.9833 indica una excelente concordancia entre las predicciones del modelo y las etiquetas reales, más allá del azar.

##### Valor P [Exactitud > Tasa de No Información]: 

El valor p < 2.2e-16 es extremadamente significativo. Esto indica que la exactitud del modelo es significativamente superior a la tasa de no información (0.3774), que representa la exactitud que se obtendría si siempre se predijera la clase más frecuente.

## Métricas por clase

##### Sensibilidad (Recall):
- AGH: 96.55% (casi todos los casos AGH reales fueron identificadas correctamente).
- CFB: 100% (todos los casos CFB reales fueron identificadas correctamente).
- CGC: 100% (todos los casos CGC reales fueron identificadas correctamente).
- CHC: 100% (todos los casos CHC reales fueron identificadas correctamente).
- HPB: 93.33% (la mayoría de los casos HPB reales fueron identificadas correctamente).

##### Especificidad:
- AGH: 100% (todos los casos que no eran AGH fueron clasificadas correctamente como no AGH).
- CFB: 100% (todos los casos que no eran CFB fueron clasificadas correctamente como no CFB).
- CGC: 99.24% (casi todos los casos que no eran CGC fueron clasificadas correctamente como no CGC).
- CHC: 99.24% (casi todos los casos que no eran CHC fueron clasificadas correctamente como no CHC).
- HPB: 100% (todos los casos que no eran HPB fueron clasificadas correctamente como no HPB).

##### Valor Predictivo Positivo (PPV):
 - AGH: 100% (todos los casos predichas como AGH realmente eran AGH).
 - CFB: 100% (todos los casos predichas como CFB realmente eran CFB).
 - CGC: 96.55% (la mayoría de las casos predichas como CGC realmente eran CGC).
 - CHC: 96.43% (la mayoría de las casos predichas como CHC realmente eran CHC).
 - HPB: 100% (todos los casos predichas como HPB realmente eran HPB).

##### Valor Predictivo Negativo (NPV):
 - AGH: 99.24% (casi todos los casos predichas como no AGH realmente no eran AGH).
 - CFB: 100% (todos los casos predichas como no CFB realmente no eran CFB).
 - CGC: 100% (todos los casos predichas como no CGC realmente no eran CGC).
 - CHC: 100% (todos los casos predichas como no CHC realmente no eran CHC).
 - HPB: 99.31% (casi todos los casos predichas como no HPB realmente no eran HPB).
 
## Resumen general:
 
Este modelo ofrece un rendimiento sobresaliente, con una alta confiabilidad y precisión en la clasificación de todas las clases. Es una opción muy sólida para su implementación, aunque una mejora en la identificación de AGH y HPB podría elevar aún más su efectividad.


# Modelo LDA (Linear Discriminant Analysis)

```{r, echo=FALSE, warning=FALSE, message=FALSE}
###########################################
#Modelo LDA (Linear Discriminant Analysis)#
###########################################
library(MASS)

#Entrenar el modelo LDA
lda_model <- MASS::lda(Clase ~ ., data = train_data)

lda_pred <- predict(lda_model, newdata = train_data)
lda_pred$x

#Realizar predicciones sobre el conjunto de prueba
lda_predictions <- predict(lda_model, newdata = test_data)
lda_predictions$x

#Obtener la predicción (predicciones de la clase)
predicted_classes <- lda_predictions$class
predicted_classes
length(predicted_classes)

#Obtener las verdaderas etiquetas (las clases reales en el conjunto de prueba)
true_classes <- as.factor(test_data$Clase)
true_classes
length(true_classes)

#Crear la matriz de confusión (predicho test vs. real test
lda_conf_matrix <- confusionMatrix(predicted_classes, true_classes)
print(lda_conf_matrix)

probabilities_lda <- predict(lda_model, newdata = test_data, type = "prob") # Obtener probabilidades

##################################################si queremos incluirlo
#Gráfico 
predict(lda_model)

library(ggplot2)
lda.data <- cbind(train_data, predict(lda_model)$x)
ggplot(lda.data, aes(x = LD1, fill = Clase)) + 
  geom_density(alpha = 0.5) + 
  theme_classic()

```

# Descripción de la Matriz de Confusión 

La matriz de confusión muestra el número de predicciones correctas e incorrectas para cada clase:

- AGH: 29 casos fueron correctamente clasificados como AGH. No hubo errores.
- CFB: 60 casos fueron correctamente clasificados como CFB. Hubo 1 falso positivo (se predijo CFB cuando era CGC).
- CGC: 27 casos fueron correctamente clasificados como CGC. Hubo 1 falso negativo (se predijo HPB cuando era CGC).
- CHC: 27 casos fueron correctamente clasificados como CHC. No hubo errores.
- HPB: 14 casos fueron correctamente clasificados como HPB. No hubo errores.

## Métricas de Evaluación

#####  Accuracy (Exactitud): 
La exactitud global del modelo es del 98.74%, lo que indica que clasificó correctamente el 98.74% de todos los casos. El intervalo de confianza del 95% (0.9553, 0.9985) sugiere una alta confianza en esta estimación.

##### Kappa: 
El coeficiente Kappa de 0.9833 indica una excelente concordancia entre las predicciones del modelo y las etiquetas reales, más allá del azar.

##### Valor P [Exactitud > Tasa de No Información]: 
El valor p < 2.2e-16 es extremadamente significativo. Esto indica que la exactitud del modelo es significativamente superior a la tasa de no información (0.3774).

## Métricas por Clase

##### Sensibilidad (Recall):
- AGH: 100% (todos los casos AGH reales fueron identificados correctamente).
- CFB: 100% (todos los casos CFB reales fueron identificados correctamente).
- CGC: 96.43% (casi todos los casos CGC reales fueron identificados correctamente).
- CHC: 100% (todos los casos CHC reales fueron identificados correctamente).
- HPB: 93.33% (la mayoría de los casos HPB reales fueron identificados correctamente).

##### Especificidad:
- AGH: 100% (todos los casos que no eran AGH fueron clasificados correctamente como no AGH).
- CFB: 98.99% (casi todos los casos que no eran CFB fueron clasificados correctamente como no CFB).
- CGC: 99.24% (casi todos los casos que no eran CGC fueron clasificados correctamente como no CGC).
- CHC: 100% (todos los casos que no eran CHC fueron clasificados correctamente como no CHC).
- HPB: 100% (todos los casos que no eran HPB fueron clasificados correctamente como no HPB).

##### Valor Predictivo Positivo (PPV):
- AGH: 100% (todos los casos predichos como AGH realmente eran AGH).
- CFB: 98.36% (casi todos los casos predichos como CFB realmente eran CFB).
- CGC: 96.43% (casi todos los casos predichos como CGC realmente eran CGC).
- CHC: 100% (todos los casos predichos como CHC realmente eran CHC).
- HPB: 100% (todos los casos predichos como HPB realmente eran HPB).

##### Valor Predictivo Negativo (NPV):
- AGH: 100% (todos los casos predichos como no AGH realmente no eran AGH).
- CFB: 100% (todos los casos predichos como no CFB realmente no eran CFB).
- CGC: 99.24% (casi todos los casos predichos como no CGC realmente no eran CGC).
- CHC: 100% (todos los casos predichos como no CHC realmente no eran CHC).
- HPB: 99.31% (casi todos los casos predichos como no HPB realmente no eran HPB).

## Resumen general:

El modelo presenta una excelente capacidad para clasificar correctamente todos los casos, con mínimos errores, y un rendimiento sobresaliente tanto en exactitud como en las métricas por clase.

# CURVAS ROC y Area Bajo la Curva (AUC) 

```{r ROC, echo=FALSE}

roc_knn <- roc(test_data$Clase, probabilities_knn[,5]) 
auc_knn <- auc(roc_knn)
cat("AUC k-NN:", auc_knn, "\n")

roc_svm_linear <- roc(test_data$Clase, probabilities_svm_linear[,5])
auc_svm_linear <- auc(roc_svm_linear)
cat("AUC SVM Lineal:", auc_svm_linear, "\n")

roc_lda <- roc(test_data$Clase, probabilities_lda$posterior[, 5])
auc_lda <- auc(roc_lda)
cat("AUC LDA:", auc_lda, "\n")


plot(roc_knn, col = "blue", main = "Curvas ROC", lwd = 2)
knn_legend <- paste("AUC k-NN:", round(auc_knn, 2))  # Redondeamos a 2 decimales, si es necesario

plot(roc_svm_linear, col = "red", add = TRUE, lwd = 2)
svm_legend <- paste("AUC SVM Lineal:", round(auc_svm_linear, 2))  # Redondeamos a 2 decimales, si es necesario

plot(roc_lda, col = "pink", add = TRUE, lwd = 2)
lda_legend <- paste("AUC LDA:", round(auc_lda, 2))  # Redondeamos a 2 decimales, si es necesario


legend("bottomright", legend = c(knn_legend, svm_legend,lda_legend),
       col = c("blue", "red","pink"), lwd = 2)


```

```{r verif, echo=FALSE}

table(test_data$Clase)

```

# Preguntas sobre las actividades


###    2. Métodos no supervisados (1 punto):

##### ¿Cuál es el motivo por el cual habéis seleccionado estas técnicas de reducción de dimensionalidad? (0,3 puntos).

##### ¿Cuál es el motivo por el cual habéis seleccionado estas técnicas de clusterización? (0,3 puntos).

##### En ambos casos, ¿qué aspectos positivos y negativos tienen cada una? (0,2 puntos).

##### En el caso de la clusterización, ¿podéis afirmar con certeza que los clústeres generados son los mejores posibles? Razonad vuestra respuesta. (0,2 puntos).

###      3. Métodos supervisados (1,75 puntos):

##### ¿Cuál es el motivo por el cual habéis seleccionado ambas técnicas de aprendizaje supervisado? ¿Cuál ha dado mejores resultados a la hora de clasificar las muestras? Razonad vuestra respuesta (1 punto).

Ver modelos supervisados variados y ver las diferencias que hay al aplicarlos. Ver las estadisiticas para contestar lo de resultados.

##### ¿Habéis considerado oportuno implementar algún método de reducción de dimensionalidad para procesar los datos antes de implementarlos en dichas técnicas? ¿Por qué? (0,5 puntos).

Para implementar QDA se necesitaria porque sale el error "Error en qda.default(x, grouping, ...): some group is too small for 'qda'" que se podría solventar reduciendo la dimensionalidad, pero finalmente no se elige este modelo.

##### ¿Qué aspectos positivos y negativos tienen cada una de las técnicas que habéis escogido? (0,25 puntos).

###      4. De estas cuatro opciones, ¿qué tipo de arquitectura de deep learning sería la más adecuada para procesar datos de expresión génica? Razonad vuestra respuesta (0,25 puntos).

a) Red de perceptrones (multiperceptron layers).
b) Redes convolucionales.
c) Redes recurrentes.
d) Redes de grafos.


\`\`\`  

<hr />

<p style="text-align: center;">
  <a href="https://www.unir.net/" style="color: #808080;"><em>https://www.unir.net/</em></a>
</p>

